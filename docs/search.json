[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exam-02",
    "section": "",
    "text": "Code\ndata = read_csv(\"./data/personality-1.csv\")\n\ndata_sub = data |&gt; \n  select(str_c(\"pers\", c(\"03\", \"07\", 11, 13, 16, 26, 36)))\n\n\n\n\nA single-factor model using seven indicators is statistically identified. This because the model contains \\(14\\) parameters \\((\\lambda_1,...,\\lambda_7, \\varepsilon_1, ..., \\varepsilon_7)\\) and the correlation matrix contains \\(\\frac{7(7+1)}{2} = \\frac{56}{2} = 28\\) unique pieces of information, also known as degrees of freedom. Hence, we have more degrees of freedom than model parameters which results in the model being statistically identified. (We have \\(28-14 = 14\\) degrees of freedom to spare 🥳). Also, following the three-indicator rule (covered in class), there are more than three indicators per latent variable.\n\n\n\n\n\nCode\nmod_01 = '\n  f1 =~ pers03 + pers07 + pers11 + pers13 + pers16 + pers26 + pers36\n'\n\ncfa_01 = cfa(mod_01, data = data_sub)\n\nfitMeasures(cfa_01) |&gt; \n  tidy() |&gt;\n  mutate(\n    measure = names,\n    value = round(x, 2),\n    .keep = \"unused\") |&gt; \n  filter(measure %in% c(\"chisq\", \"pvalue\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; \n  gt() |&gt; \n  cols_width(\n    everything() ~  px(100)\n  )\n\n\n\n\n\n\nTable 1:  Fit measures for one-factor model \n  \n    \n    \n  \n  \n    \n    \n      measure\n      value\n    \n  \n  \n    chisq\n335.48\n    pvalue\n0.00\n    cfi\n0.46\n    tli\n0.19\n    rmsea\n0.23\n    srmr\n0.17\n  \n  \n  \n\n\n\n\n\nGiven the results of Table 1, the model doesn’t seem to fit the data. Hence, no interpretation of the parameter estimates is needed.\n\n\n\n\n\nCode\nresiduals(cfa_01, type = \"standardized\")\n\n\n$type\n[1] \"standardized\"\n\n$cov\n       pers03 pers07 pers11 pers13 pers16 pers26 pers36\npers03  0.000                                          \npers07 -1.273  0.000                                   \npers11 -1.573 -1.031  0.000                            \npers13  4.594  1.889 -3.704  0.000                     \npers16 -1.317  0.380  7.996 -5.858  0.000              \npers26 -0.538 -2.414  6.839 -4.201  8.375  0.000       \npers36 -3.965  0.214  5.930 -1.999  7.634  7.289  0.000\n\n\nWe see huge standardized residuals, specifically in the correlation estimates of pers36 and the other items. This is probably due to the wording of this question and its relationship to the other items (they may be correlated). Also, the biggest misfit is happening in the estimate for the correlation between per26 and per16, more than 8 units off. However, it is unclear why is this happening with this two items that seem somewhat different (per16:generates enthusiasm in others, per26: assertive).\n\n\nCode\nmodindices(cfa_01, sort = T)\n\n\n      lhs op    rhs     mi    epc sepc.lv sepc.all sepc.nox\n34 pers16 ~~ pers26 86.315  0.505   0.505    0.449    0.449\n28 pers11 ~~ pers16 76.881  0.485   0.485    0.423    0.423\n35 pers16 ~~ pers36 69.905  0.452   0.452    0.406    0.406\n36 pers26 ~~ pers36 62.433  0.498   0.498    0.383    0.383\n29 pers11 ~~ pers26 53.372  0.471   0.471    0.352    0.352\n18 pers03 ~~ pers13 49.437  0.833   0.833    1.708    1.708\n30 pers11 ~~ pers36 39.064  0.401   0.401    0.302    0.302\n31 pers13 ~~ pers16 28.195 -0.247  -0.247   -0.358   -0.358\n32 pers13 ~~ pers26 15.656 -0.212  -0.212   -0.263   -0.263\n21 pers03 ~~ pers36 13.363 -0.182  -0.182   -0.231   -0.231\n27 pers11 ~~ pers13 12.387 -0.190  -0.190   -0.232   -0.232\n23 pers07 ~~ pers13  6.781  0.184   0.184    0.310    0.310\n25 pers07 ~~ pers26  5.680 -0.119  -0.119   -0.123   -0.123\n33 pers13 ~~ pers36  3.471 -0.101  -0.101   -0.127   -0.127\n16 pers03 ~~ pers07  2.661 -0.102  -0.102   -0.174   -0.174\n17 pers03 ~~ pers11  2.288 -0.075  -0.075   -0.093   -0.093\n19 pers03 ~~ pers16  1.514 -0.052  -0.052   -0.077   -0.077\n22 pers07 ~~ pers11  1.046 -0.052  -0.052   -0.053   -0.053\n20 pers03 ~~ pers26  0.271 -0.026  -0.026   -0.032   -0.032\n24 pers07 ~~ pers16  0.143  0.016   0.016    0.020    0.020\n26 pers07 ~~ pers36  0.046  0.011   0.011    0.011    0.011\n\n\nAs suspected, adding some correlation terms between the error component of the items will improve the fit in the model. This makes sense as some of the items are very similar and probably is easy to understand within the personality theories. Given that we have only \\(14\\) degrees of freedom left, I will add \\(10\\) correlation terms as specified in the table above. This because I am interested in maintaining an over-identified model while still be able to asses the fit of it.\n\n\n\n\n\nCode\nmod_02 = '\n  f1 =~ pers03 + pers07 + pers11 + pers13 + pers16 + pers26 + pers36\n  pers16    ~~  pers26\n  pers11    ~~  pers16\n  pers16    ~~  pers36\n  pers26    ~~  pers36\n  pers11    ~~  pers26\n  pers03    ~~  pers13\n  pers11    ~~  pers36\n  pers13    ~~  pers16\n  pers13    ~~  pers26\n  pers03    ~~  pers36\n'\n\ncfa_02 = cfa(mod_02, data = data_sub)\n\nfitMeasures(cfa_02) |&gt; \n  tidy() |&gt;\n  mutate(\n    measure = names,\n    value = round(x, 2),\n    .keep = \"unused\") |&gt; \n  filter(measure %in% c(\"chisq\", \"pvalue\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; \n  gt() |&gt; \n  cols_width(\n    everything() ~  px(100)\n  )\n\n\n\n\n\n\nTable 2:  Fit measures for one-factor model with correlated errors \n  \n    \n    \n  \n  \n    \n    \n      measure\n      value\n    \n  \n  \n    chisq\n5.92\n    pvalue\n0.21\n    cfi\n1.00\n    tli\n0.98\n    rmsea\n0.03\n    srmr\n0.02\n  \n  \n  \n\n\n\n\n\nGiven the results of Table 2, we can claim that the data does fit the specified one-factor model with some correlated errors. Comparing this table with the results of Table 1, it is clear the improvement of the original model fitted originally. Also, it is worth mentioning that these two models are nested (the model with no correlation terms is nested into the model with correlation error terms). Because of this fact, we can compare the fit of thess two nested models through a \\(\\chi^2\\) test.\n\n\nCode\nanova(cfa_01, cfa_02)\n\n\n\nChi-Squared Difference Test\n\n       Df    AIC    BIC    Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \ncfa_02  4 8607.8 8705.8   5.9217                                          \ncfa_01 14 8917.3 8974.5 335.4780     329.56 0.26949      10  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results of this test point out what we found with the goodness of fit indexes: the second model does a better job in describing the relationship between the 7 items we analyzed. Also, from a information-based criteria, the second model also has smaller values of AIC and BIC.\n\n\n\n\n\nCode\nsummary(cfa_02, standardized=T)\n\n\nlavaan 0.6.16 ended normally after 46 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n\n                                                  Used       Total\n  Number of observations                           440         452\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.922\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.205\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 =~                                                                 \n    pers03            1.000                               0.505    0.515\n    pers07            1.296    0.458    2.831    0.005    0.654    0.666\n    pers11            0.160    0.147    1.089    0.276    0.081    0.068\n    pers13            1.274    0.153    8.313    0.000    0.643    0.613\n    pers16            0.374    0.142    2.627    0.009    0.189    0.189\n    pers26            0.150    0.152    0.990    0.322    0.076    0.065\n    pers36            0.469    0.170    2.762    0.006    0.237    0.203\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .pers16 ~~                                                             \n   .pers26            0.518    0.061    8.495    0.000    0.518    0.453\n .pers11 ~~                                                             \n   .pers16            0.491    0.061    8.029    0.000    0.491    0.424\n .pers16 ~~                                                             \n   .pers36            0.446    0.061    7.321    0.000    0.446    0.397\n .pers26 ~~                                                             \n   .pers36            0.528    0.070    7.550    0.000    0.528    0.397\n .pers11 ~~                                                             \n   .pers26            0.492    0.070    7.054    0.000    0.492    0.359\n .pers03 ~~                                                             \n   .pers13            0.247    0.120    2.058    0.040    0.247    0.355\n .pers11 ~~                                                             \n   .pers36            0.423    0.069    6.159    0.000    0.423    0.313\n .pers13 ~~                                                             \n   .pers16           -0.081    0.038   -2.112    0.035   -0.081   -0.099\n   .pers26           -0.019    0.044   -0.438    0.661   -0.019   -0.020\n .pers03 ~~                                                             \n   .pers36           -0.072    0.042   -1.727    0.084   -0.072   -0.074\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .pers03            0.708    0.105    6.728    0.000    0.708    0.735\n   .pers07            0.536    0.149    3.597    0.000    0.536    0.556\n   .pers11            1.390    0.094   14.787    0.000    1.390    0.995\n   .pers13            0.687    0.152    4.525    0.000    0.687    0.624\n   .pers16            0.966    0.067   14.335    0.000    0.966    0.964\n   .pers26            1.350    0.091   14.778    0.000    1.350    0.996\n   .pers36            1.309    0.091   14.381    0.000    1.309    0.959\n    f1                0.255    0.103    2.481    0.013    1.000    1.000\n\n\nWith the above information we can claim that these items are measuring a latent factor that I’ve decided to name active worker. It is worth mentioning that some of the items are measuring the same aspect of this latent variable, that is why some correlated errors are introduced in the model."
  },
  {
    "objectID": "index.html#a",
    "href": "index.html#a",
    "title": "Exam-02",
    "section": "",
    "text": "A single-factor model using seven indicators is statistically identified. This because the model contains \\(14\\) parameters \\((\\lambda_1,...,\\lambda_7, \\varepsilon_1, ..., \\varepsilon_7)\\) and the correlation matrix contains \\(\\frac{7(7+1)}{2} = \\frac{56}{2} = 28\\) unique pieces of information, also known as degrees of freedom. Hence, we have more degrees of freedom than model parameters which results in the model being statistically identified. (We have \\(28-14 = 14\\) degrees of freedom to spare 🥳). Also, following the three-indicator rule (covered in class), there are more than three indicators per latent variable."
  },
  {
    "objectID": "index.html#b",
    "href": "index.html#b",
    "title": "Exam-02",
    "section": "",
    "text": "Code\nmod_01 = '\n  f1 =~ pers03 + pers07 + pers11 + pers13 + pers16 + pers26 + pers36\n'\n\ncfa_01 = cfa(mod_01, data = data_sub)\n\nfitMeasures(cfa_01) |&gt; \n  tidy() |&gt;\n  mutate(\n    measure = names,\n    value = round(x, 2),\n    .keep = \"unused\") |&gt; \n  filter(measure %in% c(\"chisq\", \"pvalue\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; \n  gt() |&gt; \n  cols_width(\n    everything() ~  px(100)\n  )\n\n\n\n\n\n\nTable 1:  Fit measures for one-factor model \n  \n    \n    \n  \n  \n    \n    \n      measure\n      value\n    \n  \n  \n    chisq\n335.48\n    pvalue\n0.00\n    cfi\n0.46\n    tli\n0.19\n    rmsea\n0.23\n    srmr\n0.17\n  \n  \n  \n\n\n\n\n\nGiven the results of Table 1, the model doesn’t seem to fit the data. Hence, no interpretation of the parameter estimates is needed."
  },
  {
    "objectID": "index.html#c",
    "href": "index.html#c",
    "title": "Exam-02",
    "section": "",
    "text": "Code\nresiduals(cfa_01, type = \"standardized\")\n\n\n$type\n[1] \"standardized\"\n\n$cov\n       pers03 pers07 pers11 pers13 pers16 pers26 pers36\npers03  0.000                                          \npers07 -1.273  0.000                                   \npers11 -1.573 -1.031  0.000                            \npers13  4.594  1.889 -3.704  0.000                     \npers16 -1.317  0.380  7.996 -5.858  0.000              \npers26 -0.538 -2.414  6.839 -4.201  8.375  0.000       \npers36 -3.965  0.214  5.930 -1.999  7.634  7.289  0.000\n\n\nWe see huge standardized residuals, specifically in the correlation estimates of pers36 and the other items. This is probably due to the wording of this question and its relationship to the other items (they may be correlated). Also, the biggest misfit is happening in the estimate for the correlation between per26 and per16, more than 8 units off. However, it is unclear why is this happening with this two items that seem somewhat different (per16:generates enthusiasm in others, per26: assertive).\n\n\nCode\nmodindices(cfa_01, sort = T)\n\n\n      lhs op    rhs     mi    epc sepc.lv sepc.all sepc.nox\n34 pers16 ~~ pers26 86.315  0.505   0.505    0.449    0.449\n28 pers11 ~~ pers16 76.881  0.485   0.485    0.423    0.423\n35 pers16 ~~ pers36 69.905  0.452   0.452    0.406    0.406\n36 pers26 ~~ pers36 62.433  0.498   0.498    0.383    0.383\n29 pers11 ~~ pers26 53.372  0.471   0.471    0.352    0.352\n18 pers03 ~~ pers13 49.437  0.833   0.833    1.708    1.708\n30 pers11 ~~ pers36 39.064  0.401   0.401    0.302    0.302\n31 pers13 ~~ pers16 28.195 -0.247  -0.247   -0.358   -0.358\n32 pers13 ~~ pers26 15.656 -0.212  -0.212   -0.263   -0.263\n21 pers03 ~~ pers36 13.363 -0.182  -0.182   -0.231   -0.231\n27 pers11 ~~ pers13 12.387 -0.190  -0.190   -0.232   -0.232\n23 pers07 ~~ pers13  6.781  0.184   0.184    0.310    0.310\n25 pers07 ~~ pers26  5.680 -0.119  -0.119   -0.123   -0.123\n33 pers13 ~~ pers36  3.471 -0.101  -0.101   -0.127   -0.127\n16 pers03 ~~ pers07  2.661 -0.102  -0.102   -0.174   -0.174\n17 pers03 ~~ pers11  2.288 -0.075  -0.075   -0.093   -0.093\n19 pers03 ~~ pers16  1.514 -0.052  -0.052   -0.077   -0.077\n22 pers07 ~~ pers11  1.046 -0.052  -0.052   -0.053   -0.053\n20 pers03 ~~ pers26  0.271 -0.026  -0.026   -0.032   -0.032\n24 pers07 ~~ pers16  0.143  0.016   0.016    0.020    0.020\n26 pers07 ~~ pers36  0.046  0.011   0.011    0.011    0.011\n\n\nAs suspected, adding some correlation terms between the error component of the items will improve the fit in the model. This makes sense as some of the items are very similar and probably is easy to understand within the personality theories. Given that we have only \\(14\\) degrees of freedom left, I will add \\(10\\) correlation terms as specified in the table above. This because I am interested in maintaining an over-identified model while still be able to asses the fit of it."
  },
  {
    "objectID": "index.html#d",
    "href": "index.html#d",
    "title": "Exam-02",
    "section": "",
    "text": "Code\nmod_02 = '\n  f1 =~ pers03 + pers07 + pers11 + pers13 + pers16 + pers26 + pers36\n  pers16    ~~  pers26\n  pers11    ~~  pers16\n  pers16    ~~  pers36\n  pers26    ~~  pers36\n  pers11    ~~  pers26\n  pers03    ~~  pers13\n  pers11    ~~  pers36\n  pers13    ~~  pers16\n  pers13    ~~  pers26\n  pers03    ~~  pers36\n'\n\ncfa_02 = cfa(mod_02, data = data_sub)\n\nfitMeasures(cfa_02) |&gt; \n  tidy() |&gt;\n  mutate(\n    measure = names,\n    value = round(x, 2),\n    .keep = \"unused\") |&gt; \n  filter(measure %in% c(\"chisq\", \"pvalue\", \"cfi\", \"tli\", \"rmsea\", \"srmr\")) |&gt; \n  gt() |&gt; \n  cols_width(\n    everything() ~  px(100)\n  )\n\n\n\n\n\n\nTable 2:  Fit measures for one-factor model with correlated errors \n  \n    \n    \n  \n  \n    \n    \n      measure\n      value\n    \n  \n  \n    chisq\n5.92\n    pvalue\n0.21\n    cfi\n1.00\n    tli\n0.98\n    rmsea\n0.03\n    srmr\n0.02\n  \n  \n  \n\n\n\n\n\nGiven the results of Table 2, we can claim that the data does fit the specified one-factor model with some correlated errors. Comparing this table with the results of Table 1, it is clear the improvement of the original model fitted originally. Also, it is worth mentioning that these two models are nested (the model with no correlation terms is nested into the model with correlation error terms). Because of this fact, we can compare the fit of thess two nested models through a \\(\\chi^2\\) test.\n\n\nCode\nanova(cfa_01, cfa_02)\n\n\n\nChi-Squared Difference Test\n\n       Df    AIC    BIC    Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \ncfa_02  4 8607.8 8705.8   5.9217                                          \ncfa_01 14 8917.3 8974.5 335.4780     329.56 0.26949      10  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results of this test point out what we found with the goodness of fit indexes: the second model does a better job in describing the relationship between the 7 items we analyzed. Also, from a information-based criteria, the second model also has smaller values of AIC and BIC."
  },
  {
    "objectID": "index.html#e",
    "href": "index.html#e",
    "title": "Exam-02",
    "section": "",
    "text": "Code\nsummary(cfa_02, standardized=T)\n\n\nlavaan 0.6.16 ended normally after 46 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        24\n\n                                                  Used       Total\n  Number of observations                           440         452\n\nModel Test User Model:\n                                                      \n  Test statistic                                 5.922\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.205\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f1 =~                                                                 \n    pers03            1.000                               0.505    0.515\n    pers07            1.296    0.458    2.831    0.005    0.654    0.666\n    pers11            0.160    0.147    1.089    0.276    0.081    0.068\n    pers13            1.274    0.153    8.313    0.000    0.643    0.613\n    pers16            0.374    0.142    2.627    0.009    0.189    0.189\n    pers26            0.150    0.152    0.990    0.322    0.076    0.065\n    pers36            0.469    0.170    2.762    0.006    0.237    0.203\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .pers16 ~~                                                             \n   .pers26            0.518    0.061    8.495    0.000    0.518    0.453\n .pers11 ~~                                                             \n   .pers16            0.491    0.061    8.029    0.000    0.491    0.424\n .pers16 ~~                                                             \n   .pers36            0.446    0.061    7.321    0.000    0.446    0.397\n .pers26 ~~                                                             \n   .pers36            0.528    0.070    7.550    0.000    0.528    0.397\n .pers11 ~~                                                             \n   .pers26            0.492    0.070    7.054    0.000    0.492    0.359\n .pers03 ~~                                                             \n   .pers13            0.247    0.120    2.058    0.040    0.247    0.355\n .pers11 ~~                                                             \n   .pers36            0.423    0.069    6.159    0.000    0.423    0.313\n .pers13 ~~                                                             \n   .pers16           -0.081    0.038   -2.112    0.035   -0.081   -0.099\n   .pers26           -0.019    0.044   -0.438    0.661   -0.019   -0.020\n .pers03 ~~                                                             \n   .pers36           -0.072    0.042   -1.727    0.084   -0.072   -0.074\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .pers03            0.708    0.105    6.728    0.000    0.708    0.735\n   .pers07            0.536    0.149    3.597    0.000    0.536    0.556\n   .pers11            1.390    0.094   14.787    0.000    1.390    0.995\n   .pers13            0.687    0.152    4.525    0.000    0.687    0.624\n   .pers16            0.966    0.067   14.335    0.000    0.966    0.964\n   .pers26            1.350    0.091   14.778    0.000    1.350    0.996\n   .pers36            1.309    0.091   14.381    0.000    1.309    0.959\n    f1                0.255    0.103    2.481    0.013    1.000    1.000\n\n\nWith the above information we can claim that these items are measuring a latent factor that I’ve decided to name active worker. It is worth mentioning that some of the items are measuring the same aspect of this latent variable, that is why some correlated errors are introduced in the model."
  },
  {
    "objectID": "index.html#a-1",
    "href": "index.html#a-1",
    "title": "Exam-02",
    "section": "a)",
    "text": "a)\n\n\nCode\ncor_mat = data |&gt; \n  drop_na() |&gt; \n  cor()\n\ncortest.bartlett(cor_mat, n = nrow(data |&gt; drop_na()))\n\n\n$chisq\n[1] 6730.363\n\n$p.value\n[1] 0\n\n$df\n[1] 946\n\n\nCode\nKMO(data |&gt; drop_na())\n\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = drop_na(data))\nOverall MSA =  0.84\nMSA for each item = \npers01 pers02 pers03 pers04 pers05 pers06 pers07 pers08 pers09 pers10 pers11 \n  0.83   0.76   0.91   0.87   0.85   0.85   0.89   0.90   0.79   0.80   0.79 \npers12 pers13 pers14 pers15 pers16 pers17 pers18 pers19 pers20 pers21 pers22 \n  0.76   0.87   0.87   0.76   0.85   0.84   0.83   0.81   0.82   0.86   0.87 \npers23 pers24 pers25 pers26 pers27 pers28 pers29 pers30 pers31 pers32 pers33 \n  0.90   0.87   0.75   0.84   0.84   0.90   0.79   0.68   0.84   0.85   0.80 \npers34 pers35 pers36 pers37 pers38 pers39 pers40 pers41 pers42 pers43 pers44 \n  0.79   0.71   0.89   0.84   0.88   0.88   0.79   0.74   0.88   0.89   0.66 \n\n\nGiven the results of this two test, we can conclude that the data is suitable for factor analysis. First, the Bartlett’s test of sphericity provides evidence that our correlation matrix is different from the identity matrix and that there is an intercorrelation between variables that can be explained by common factors. Then, the KMO provides evidence that our data suitable for this analysis, with an overall measure of sampling adequacy of \\(0.84\\), which is great"
  },
  {
    "objectID": "index.html#b-1",
    "href": "index.html#b-1",
    "title": "Exam-02",
    "section": "b)",
    "text": "b)\n\n\nCode\nfa.parallel(data, fm = \"ml\", n.obs = nrow(data |&gt; drop_na()))\n\n\n\n\n\nParallel analysis suggests that the number of factors =  6  and the number of components =  6 \n\n\nCode\nscree(data |&gt; drop_na())\n\n\n\n\n\nBoth the parallel analysis and the scree-plot suggest that the number of factors to be extracted is \\(6\\).\n\n\nCode\nefa_01 = fa(cor_mat, \n            nfactors = 6, \n            n.obs = nrow(data |&gt; drop_na()), \n            fm = \"ml\", \n            rotate = \"none\")\n\nsummary(efa_01)\n\n\n\nFactor analysis with Call: fa(r = cor_mat, nfactors = 6, n.obs = nrow(drop_na(data)), rotate = \"none\", \n    fm = \"ml\")\n\nTest of the hypothesis that 6 factors are sufficient.\nThe degrees of freedom for the model is 697  and the objective function was  3.26 \nThe number of observations was  433  with Chi Square =  1344.09  with prob &lt;  1.7e-43 \n\nThe root mean square of the residuals (RMSA) is  0.04 \nThe df corrected root mean square of the residuals is  0.04 \n\nTucker Lewis Index of factoring reliability =  0.846\nRMSEA index =  0.046  and the 10 % confidence intervals are  0.043 0.05\nBIC =  -2887.21\n\n\nBased on the goodness-of-fit indices, this model seems to be doing a decent job in explaining the intercorrelation between the items. Even thought the \\(\\chi^2\\) test is significant, the other measures to evaluate how good the data fits the model are withing acceptable ranges (RMSA = 0.04, TLI = 0.846, RMSEA = 0.046)."
  },
  {
    "objectID": "index.html#c-1",
    "href": "index.html#c-1",
    "title": "Exam-02",
    "section": "c)",
    "text": "c)\nBecause these items are similar, I would prefer an oblique rotation to allow correlation between the latent factors.\n\n\nCode\nefa_02 = fa(cor_mat, \n            nfactors = 6, \n            n.obs = nrow(data |&gt; drop_na()), \n            fm=\"ml\", \n            rotate = \"promax\")\n\nefa_02$loadings\n\n\n\nLoadings:\n       ML2    ML1    ML3    ML5    ML4    ML6   \npers01        -0.663  0.210  0.333 -0.106  0.313\npers02  0.187 -0.129  0.193 -0.451         0.364\npers03  0.664                0.122         0.129\npers04         0.122  0.512 -0.147        -0.120\npers05  0.120 -0.199                0.535       \npers06  0.128  0.728                0.119       \npers07  0.190                0.502              \npers08 -0.620                              0.165\npers09         0.166 -0.716                0.162\npers10                              0.362  0.275\npers11               -0.267                0.485\npers12                      -0.321         0.196\npers13  0.528         0.104  0.253              \npers14                0.676                     \npers15  0.118  0.138        -0.140  0.530       \npers16        -0.225         0.130  0.139  0.528\npers17 -0.138        -0.107  0.405         0.139\npers18 -0.725                0.149              \npers19                0.631  0.126  0.130 -0.145\npers20        -0.114  0.156         0.471  0.178\npers21         0.892                0.108 -0.162\npers22                       0.671              \npers23 -0.616               -0.123              \npers24               -0.626                0.201\npers25                      -0.135  0.515  0.201\npers26  0.131 -0.319 -0.181 -0.204         0.512\npers27         0.269        -0.316         0.166\npers28  0.597                0.161              \npers29                0.452                0.127\npers30         0.150                0.530       \npers31         0.700         0.188        -0.117\npers32  0.156                0.664              \npers33  0.655  0.133               -0.108  0.249\npers34         0.143 -0.638                0.207\npers35         0.224               -0.205       \npers36        -0.512         0.225 -0.101  0.427\npers37 -0.228 -0.107        -0.400         0.234\npers38  0.500                              0.195\npers39         0.384  0.437  0.135              \npers40         0.139                0.621       \npers41                             -0.387  0.105\npers42  0.120         0.103  0.595              \npers43 -0.525         0.164                0.236\npers44         0.118                0.491       \n\n                 ML2   ML1   ML3   ML5   ML4   ML6\nSS loadings    3.636 3.222 3.193 2.780 2.441 1.936\nProportion Var 0.083 0.073 0.073 0.063 0.055 0.044\nCumulative Var 0.083 0.156 0.228 0.292 0.347 0.391\n\n\nWe see some of the largest factor loading to be negative. I attribute this to the nature of the items in this test. Given that these items are trying to measure personality traits, it is tenable to have items with negative loads on some of the traits and still have a meaningful interpretation. Take, for example, item 8 careless. A negative load of this item on some of the latent variable would indicate that the person is not careless, which means that this worker is careful. In other words, these items may be in reverse coding.\nWith the loading of the rotated solution, I can provide the following interpretation of the latent factors:\n\nFactor 1 (Calmn): This factor might capture emotional stability and calmness, with items like “emotionally stable,” “calm in tense situations,” and “relaxed” showing high loadings.\nFactor 2 (Reliable): Indicated by high loadings on items like “does a thorough job,” “reliable,” and “perseveres,” this factor likely measures conscientiousness, work ethic, and reliability.\nFactor 3 (Worry): This factor might capture worry, with items like “depressed,” “not relaxed,” and “worries” showing high loadings.\nFactor 4 (Creative): Suggested by high loadings on “original,” “imaginative,” and “values artistic experiences,” this factor seems to represent openness to experience and creativity.\nFactor 5 (Trustworthy): Indicated by high loadings on items like “trusting,” “considerate,” and “co-operative,” this factor likely measures to what extent the worker is trustworthy.\nFactor 6 (Leadership): This factor might be related to assertiveness and leadership qualities, as indicated by items like “assertive,” “generates enthusiasm in others,” and “sophisticated in art & music” having high loadings."
  },
  {
    "objectID": "index.html#d-1",
    "href": "index.html#d-1",
    "title": "Exam-02",
    "section": "d)",
    "text": "d)\n\n\nCode\nefa_02$communalities |&gt; \n  tidy() |&gt; \n  mutate(\n    item = parse_number(names),\n    communality = x,\n    .keep = \"unused\"\n  ) |&gt; \n  ggplot(aes(x = item, y = communality)) +\n  geom_col(fill = \"tomato4\") +\n  ylim(c(0,1)) +\n  theme_bw()\n\n\n\n\n\nFigure 1: Communalities\n\n\n\n\nLow communalities in a factor analysis, like the ones observed in Figure 1 (specifically, pers10, pers12, pers17, pers35), suggest that these items are not well explained by the underlined extracted factors. In this context, this can mean that these specific items do not align well with the underlying constructs we are trying to measure with this instrument. In other words, these items might be measuring aspects of personality that are not captured by the six factors we’ve identified, or they may be less relevant or inconsistent in the context of the other items and factors in this questionnaire."
  },
  {
    "objectID": "index.html#e-1",
    "href": "index.html#e-1",
    "title": "Exam-02",
    "section": "e)",
    "text": "e)\nFactor Analysis (FA) is more appropriate for this dataset than Principal Component Analysis (PCA) because FA seeks to identify latent variables that explain observed variables, which is suitable for psychological and personality data, like the one we have. FA models the underlying structure that explains correlations between items, focusing on shared variance. In contrast, PCA maximizes total variance, treating all variance as equally important. PCA would provide a different perspective by combining items into components based on total variance, potentially mixing measurement and error variances, which might not be as meaningful for understanding underlying personality constructs. However, it all depends on the objective of the research."
  },
  {
    "objectID": "index.html#f",
    "href": "index.html#f",
    "title": "Exam-02",
    "section": "f)",
    "text": "f)\n\n\nCode\ndata |&gt; \n  drop_na() |&gt; \n  skimr::skim() |&gt; \n  as_tibble() |&gt; \n  select(skim_variable, numeric.sd) |&gt; \n  sample_n(10) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      skim_variable\n      numeric.sd\n    \n  \n  \n    pers05\n1.0205579\n    pers44\n1.2292195\n    pers38\n1.0733263\n    pers07\n0.9865824\n    pers12\n1.2655964\n    pers11\n1.1773949\n    pers42\n0.9923958\n    pers03\n0.9766104\n    pers29\n1.1612010\n    pers37\n1.2635968\n  \n  \n  \n\n\n\n\nPerforming an exploratory factor analysis using the covariance matrix is reasonable for this dataset, as the items (pers01 to pers44) are on the same 5-point scale and have similar standard deviations (and hence variances). This similarity in scaling and variance allows for a meaningful comparison of the covariance among items. The results of the factor analysis using the covariance matrix would focus more directly on the shared variances in their original scale, potentially providing insights that align more closely with the actual variance observed in the data. In conclusion, the result will be very similar."
  },
  {
    "objectID": "index.html#a-2",
    "href": "index.html#a-2",
    "title": "Exam-02",
    "section": "a)",
    "text": "a)\nBy definition of our model, we have the following:\n\\[\n\\hat{\\Sigma} = \\hat{\\Lambda}\\hat{\\Lambda}^{t} + \\hat{\\Psi}\n\\]\nUsing linear algebra, we can compute the fitted correlation matrix as follows:\n\\[\n\\begin{align*}\n\\hat{\\Sigma} &= \\begin{bmatrix}\n0.4 & 0.8\\\\\n0.7 & -0.4\\\\\n0.1 & 0.7\\\\\n0.5 & -0.7\\\\\n0.5 & -0.3\\\\\n0.5 & 0.2\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n0.4 & 0.8\\\\\n0.7 & -0.4\\\\\n0.1 & 0.7\\\\\n0.5 & -0.7\\\\\n0.5 & -0.3\\\\\n0.5 & 0.2\n\\end{bmatrix}^t + \\begin{bmatrix}\n.20 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & .35 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & .50 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & .26 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & .66 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & .80\n\\end{bmatrix} \\\\\n\\\\\n&= \\begin{bmatrix}\n(0.4 \\times 0.4 + 0.8 \\times 0.8) & (0.4 \\times 0.7 + 0.8 \\times -0.4) & \\cdots & (0.4 \\times 0.5 + 0.8 \\times 0.2) \\\\\n(0.7 \\times 0.4 + -0.4 \\times 0.8) & (0.7 \\times 0.7 + -0.4 \\times -0.4) & \\cdots & (0.7 \\times 0.5 + -0.4 \\times 0.2) \\\\\n\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\\\\n(0.5 \\times 0.4 + 0.2 \\times 0.8) & (0.5 \\times 0.7 + 0.2 \\times -0.4) & \\cdots & (0.5 \\times 0.5 + 0.2 \\times 0.2) \\\\\n\\end{bmatrix}\\\\ &+ \\begin{bmatrix}\n.20 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & .35 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & .50 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & .26 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & .66 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & .80\n\\end{bmatrix} \\\\\n\\\\\n&= \\begin{bmatrix}\n0.80 & -0.04 & 0.60 & -0.36 & -0.04 & 0.32 \\\\\n-0.04 & 0.65 & -0.07 & 0.49 & 0.53 & 0.38 \\\\\n0.60 & -0.07 & 0.50 & -0.44 & -0.16 & 0.18 \\\\\n-0.36 & 0.49 & -0.44 & 0.74 & 0.66 & 0.14 \\\\\n-0.04 & 0.53 & -0.16 & 0.66 & 0.34 & 0.26 \\\\\n0.32 & 0.38 & 0.18 & 0.14 & 0.26 & 0.29 \\\\\n\\end{bmatrix} + \\begin{bmatrix}\n.20 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & .35 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & .50 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & .26 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & .66 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & .80\n\\end{bmatrix} \\\\\n\\\\\n&= \\begin{bmatrix}\n1.00 & -0.04 & 0.60 & -0.36 & -0.04 & 0.32 \\\\\n-0.04 & 1.00 & -0.21 & 0.63 & 0.47 & 0.20 \\\\\n0.60 & -0.21 & 1.00 & -0.44 & -0.16 & 0.18 \\\\\n-0.36 & 0.63 & -0.44 & 1.00 & 0.46 & 0.06 \\\\\n-0.04 & 0.47 & -0.16 & 0.46 & 1.00 & 0.14 \\\\\n0.32 & 0.20 & 0.18 & 0.06 & 0.14 & 1.00 \\\\\n\\end{bmatrix}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "index.html#b-2",
    "href": "index.html#b-2",
    "title": "Exam-02",
    "section": "b)",
    "text": "b)\nFor each item, the communality can be computed as follows:\n\nVariable 1: \\((0.4)^2 + (0.8)^2 = 0.80\\)\nVariable 2: \\((0.7)^2 + (-0.4)^2 = 0.65\\)\nVariable 3: \\((0.1)^2 + (0.7)^2 = 0.50\\)\nVariable 4: \\((0.5)^2 + (-0.7)^2 = 0.74\\)\nVariable 5: \\((0.5)^2 + (-0.3)^2 = 0.34\\)\nVariable 6: \\((0.4)^2 + (-0.2)^2 = 0.20\\)\n\nThe uniqueness can be computed as the complement of the communality, this because the correlation matrix was used to perform the factor analysis:\n\nVariable 1: \\(1 - 0.80 = 0.20\\)\nVariable 2: \\(1 - 0.65 = 0.35\\)\nVariable 3: \\(1 - 0.50 = 0.50\\)\nVariable 4: \\(1 - 0.74 = 0.26\\)\nVariable 5: \\(1 - 0.34 = 0.66\\)\nVariable 6: \\(1 - 0.20 = 0.80\\)"
  }
]